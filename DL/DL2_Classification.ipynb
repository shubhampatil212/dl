{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fRGY3TUZ-GP"
      },
      "source": [
        "### DL LAB 2A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HU2EOx01WXqT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1jgaZbEuWYUl"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\"\n",
        "columns = ['letter', 'x-box', 'y-box', 'width', 'height', 'onpix', 'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar',\n",
        "           'x2ybr', 'xy2br', 'x-ege', 'xegvy', 'y-ege', 'yegvx']\n",
        "data = pd.read_csv(url, names=columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8a4BM7c39V2l"
      },
      "outputs": [],
      "source": [
        "# 2. Separate features and labels\n",
        "X = data.drop('letter', axis=1).values\n",
        "y = data['letter'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6j23oTLfWrED"
      },
      "outputs": [],
      "source": [
        "# 3. Encode labels (A-Z -> 0-25)\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "y_categorical = to_categorical(y_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mFdOm1qKWzwg"
      },
      "outputs": [],
      "source": [
        "# 4. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42, stratify=y_categorical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "m7J9a8xkW1W2"
      },
      "outputs": [],
      "source": [
        "# 5. Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwJggL1YW3Er",
        "outputId": "fe201f2c-380c-4a73-e96e-cff7f79d20e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# 6. Build the DNN model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(16,)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(26, activation='softmax')  # 26 letters A-Z\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWA40Dr4W5FE",
        "outputId": "c636bf99-4051-483d-fb51-f791ae08ca2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.4298 - loss: 2.1062 - val_accuracy: 0.7644 - val_loss: 0.8268\n",
            "Epoch 2/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7821 - loss: 0.7540 - val_accuracy: 0.8294 - val_loss: 0.5878\n",
            "Epoch 3/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8427 - loss: 0.5478 - val_accuracy: 0.8525 - val_loss: 0.4802\n",
            "Epoch 4/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8655 - loss: 0.4526 - val_accuracy: 0.8813 - val_loss: 0.3901\n",
            "Epoch 5/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8961 - loss: 0.3724 - val_accuracy: 0.8888 - val_loss: 0.3520\n",
            "Epoch 6/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9099 - loss: 0.3052 - val_accuracy: 0.9137 - val_loss: 0.2905\n",
            "Epoch 7/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9245 - loss: 0.2622 - val_accuracy: 0.9137 - val_loss: 0.2665\n",
            "Epoch 8/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9283 - loss: 0.2429 - val_accuracy: 0.9187 - val_loss: 0.2560\n",
            "Epoch 9/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9382 - loss: 0.2108 - val_accuracy: 0.9250 - val_loss: 0.2313\n",
            "Epoch 10/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9470 - loss: 0.1895 - val_accuracy: 0.9337 - val_loss: 0.2161\n",
            "Epoch 11/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9487 - loss: 0.1711 - val_accuracy: 0.9325 - val_loss: 0.2038\n",
            "Epoch 12/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9532 - loss: 0.1583 - val_accuracy: 0.9300 - val_loss: 0.2104\n",
            "Epoch 13/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9542 - loss: 0.1473 - val_accuracy: 0.9369 - val_loss: 0.1894\n",
            "Epoch 14/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9615 - loss: 0.1297 - val_accuracy: 0.9456 - val_loss: 0.1775\n",
            "Epoch 15/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9659 - loss: 0.1197 - val_accuracy: 0.9463 - val_loss: 0.1761\n",
            "Epoch 16/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9665 - loss: 0.1142 - val_accuracy: 0.9425 - val_loss: 0.1792\n",
            "Epoch 17/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9678 - loss: 0.1030 - val_accuracy: 0.9438 - val_loss: 0.1666\n",
            "Epoch 18/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9697 - loss: 0.0959 - val_accuracy: 0.9463 - val_loss: 0.1706\n",
            "Epoch 19/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9722 - loss: 0.0920 - val_accuracy: 0.9438 - val_loss: 0.1620\n",
            "Epoch 20/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9794 - loss: 0.0763 - val_accuracy: 0.9431 - val_loss: 0.1682\n",
            "Epoch 21/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9771 - loss: 0.0768 - val_accuracy: 0.9438 - val_loss: 0.1652\n",
            "Epoch 22/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9803 - loss: 0.0674 - val_accuracy: 0.9506 - val_loss: 0.1616\n",
            "Epoch 23/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9823 - loss: 0.0657 - val_accuracy: 0.9556 - val_loss: 0.1465\n",
            "Epoch 24/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9781 - loss: 0.0654 - val_accuracy: 0.9456 - val_loss: 0.1580\n",
            "Epoch 25/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9813 - loss: 0.0603 - val_accuracy: 0.9506 - val_loss: 0.1516\n",
            "Epoch 26/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9851 - loss: 0.0532 - val_accuracy: 0.9500 - val_loss: 0.1498\n",
            "Epoch 27/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9838 - loss: 0.0534 - val_accuracy: 0.9525 - val_loss: 0.1510\n",
            "Epoch 28/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9841 - loss: 0.0517 - val_accuracy: 0.9481 - val_loss: 0.1676\n",
            "Epoch 29/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9863 - loss: 0.0475 - val_accuracy: 0.9544 - val_loss: 0.1360\n",
            "Epoch 30/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9875 - loss: 0.0426 - val_accuracy: 0.9475 - val_loss: 0.1573\n",
            "Epoch 31/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9876 - loss: 0.0452 - val_accuracy: 0.9469 - val_loss: 0.1650\n",
            "Epoch 32/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9881 - loss: 0.0408 - val_accuracy: 0.9481 - val_loss: 0.1682\n",
            "Epoch 33/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9889 - loss: 0.0394 - val_accuracy: 0.9538 - val_loss: 0.1384\n",
            "Epoch 34/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9897 - loss: 0.0373 - val_accuracy: 0.9481 - val_loss: 0.1751\n",
            "Epoch 35/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9915 - loss: 0.0335 - val_accuracy: 0.9519 - val_loss: 0.1447\n",
            "Epoch 36/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9927 - loss: 0.0292 - val_accuracy: 0.9544 - val_loss: 0.1454\n",
            "Epoch 37/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9901 - loss: 0.0350 - val_accuracy: 0.9463 - val_loss: 0.1779\n",
            "Epoch 38/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9928 - loss: 0.0280 - val_accuracy: 0.9500 - val_loss: 0.1564\n",
            "Epoch 39/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9865 - loss: 0.0401 - val_accuracy: 0.9500 - val_loss: 0.1543\n",
            "Epoch 40/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9929 - loss: 0.0249 - val_accuracy: 0.9500 - val_loss: 0.1487\n",
            "Epoch 41/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9931 - loss: 0.0270 - val_accuracy: 0.9425 - val_loss: 0.1742\n",
            "Epoch 42/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9934 - loss: 0.0251 - val_accuracy: 0.9550 - val_loss: 0.1472\n",
            "Epoch 43/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9959 - loss: 0.0200 - val_accuracy: 0.9581 - val_loss: 0.1477\n",
            "Epoch 44/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9915 - loss: 0.0293 - val_accuracy: 0.9481 - val_loss: 0.1695\n",
            "Epoch 45/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9942 - loss: 0.0215 - val_accuracy: 0.9513 - val_loss: 0.1548\n",
            "Epoch 46/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9920 - loss: 0.0253 - val_accuracy: 0.9575 - val_loss: 0.1316\n",
            "Epoch 47/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9952 - loss: 0.0183 - val_accuracy: 0.9525 - val_loss: 0.1558\n",
            "Epoch 48/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9945 - loss: 0.0215 - val_accuracy: 0.9563 - val_loss: 0.1498\n",
            "Epoch 49/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9945 - loss: 0.0215 - val_accuracy: 0.9494 - val_loss: 0.1553\n",
            "Epoch 50/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9958 - loss: 0.0167 - val_accuracy: 0.9550 - val_loss: 0.1372\n",
            "Epoch 51/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9944 - loss: 0.0220 - val_accuracy: 0.9550 - val_loss: 0.1595\n",
            "Epoch 52/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9948 - loss: 0.0208 - val_accuracy: 0.9337 - val_loss: 0.2598\n",
            "Epoch 53/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9929 - loss: 0.0267 - val_accuracy: 0.9550 - val_loss: 0.1458\n",
            "Epoch 54/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9963 - loss: 0.0148 - val_accuracy: 0.9544 - val_loss: 0.1556\n",
            "Epoch 55/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9968 - loss: 0.0121 - val_accuracy: 0.9563 - val_loss: 0.1491\n",
            "Epoch 56/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9957 - loss: 0.0178 - val_accuracy: 0.9525 - val_loss: 0.1597\n",
            "Epoch 57/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9960 - loss: 0.0173 - val_accuracy: 0.9500 - val_loss: 0.1756\n",
            "Epoch 58/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9961 - loss: 0.0148 - val_accuracy: 0.9525 - val_loss: 0.1735\n",
            "Epoch 59/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9969 - loss: 0.0129 - val_accuracy: 0.9494 - val_loss: 0.1847\n",
            "Epoch 60/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9974 - loss: 0.0111 - val_accuracy: 0.9594 - val_loss: 0.1600\n",
            "Epoch 61/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9969 - loss: 0.0133 - val_accuracy: 0.9556 - val_loss: 0.1772\n",
            "Epoch 62/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9955 - loss: 0.0164 - val_accuracy: 0.9494 - val_loss: 0.1948\n",
            "Epoch 63/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9967 - loss: 0.0133 - val_accuracy: 0.9494 - val_loss: 0.2052\n",
            "Epoch 64/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9971 - loss: 0.0113 - val_accuracy: 0.9500 - val_loss: 0.1810\n",
            "Epoch 65/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9971 - loss: 0.0108 - val_accuracy: 0.9575 - val_loss: 0.1596\n",
            "Epoch 66/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9983 - loss: 0.0074 - val_accuracy: 0.9600 - val_loss: 0.1614\n",
            "Epoch 67/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9959 - loss: 0.0154 - val_accuracy: 0.9488 - val_loss: 0.1665\n",
            "Epoch 68/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9948 - loss: 0.0177 - val_accuracy: 0.9588 - val_loss: 0.1599\n",
            "Epoch 69/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9957 - loss: 0.0132 - val_accuracy: 0.9525 - val_loss: 0.1831\n",
            "Epoch 70/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9976 - loss: 0.0120 - val_accuracy: 0.9550 - val_loss: 0.1669\n",
            "Epoch 71/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9975 - loss: 0.0093 - val_accuracy: 0.9588 - val_loss: 0.1615\n",
            "Epoch 72/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9989 - loss: 0.0056 - val_accuracy: 0.9563 - val_loss: 0.1616\n",
            "Epoch 73/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0073 - val_accuracy: 0.9538 - val_loss: 0.1853\n",
            "Epoch 74/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9920 - loss: 0.0279 - val_accuracy: 0.9519 - val_loss: 0.1741\n",
            "Epoch 75/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9970 - loss: 0.0130 - val_accuracy: 0.9613 - val_loss: 0.1585\n",
            "Epoch 76/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9968 - loss: 0.0107 - val_accuracy: 0.9594 - val_loss: 0.1475\n",
            "Epoch 77/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9991 - loss: 0.0044 - val_accuracy: 0.9594 - val_loss: 0.1555\n",
            "Epoch 78/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9992 - loss: 0.0059 - val_accuracy: 0.9531 - val_loss: 0.1807\n",
            "Epoch 79/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9910 - loss: 0.0313 - val_accuracy: 0.9506 - val_loss: 0.1918\n",
            "Epoch 80/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9963 - loss: 0.0125 - val_accuracy: 0.9538 - val_loss: 0.1757\n",
            "Epoch 81/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9982 - loss: 0.0069 - val_accuracy: 0.9550 - val_loss: 0.1679\n",
            "Epoch 82/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9992 - loss: 0.0050 - val_accuracy: 0.9525 - val_loss: 0.1872\n",
            "Epoch 83/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9995 - loss: 0.0042 - val_accuracy: 0.9575 - val_loss: 0.1635\n",
            "Epoch 84/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9959 - loss: 0.0167 - val_accuracy: 0.9531 - val_loss: 0.2051\n",
            "Epoch 85/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9951 - loss: 0.0167 - val_accuracy: 0.9481 - val_loss: 0.1995\n",
            "Epoch 86/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9989 - loss: 0.0073 - val_accuracy: 0.9575 - val_loss: 0.1638\n",
            "Epoch 87/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9992 - loss: 0.0039 - val_accuracy: 0.9563 - val_loss: 0.1805\n",
            "Epoch 88/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9954 - loss: 0.0143 - val_accuracy: 0.9463 - val_loss: 0.2057\n",
            "Epoch 89/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9989 - loss: 0.0053 - val_accuracy: 0.9556 - val_loss: 0.1806\n",
            "Epoch 90/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9986 - loss: 0.0071 - val_accuracy: 0.9488 - val_loss: 0.2074\n",
            "Epoch 91/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9964 - loss: 0.0122 - val_accuracy: 0.9550 - val_loss: 0.1908\n",
            "Epoch 92/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9974 - loss: 0.0090 - val_accuracy: 0.9506 - val_loss: 0.1831\n",
            "Epoch 93/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9983 - loss: 0.0066 - val_accuracy: 0.9550 - val_loss: 0.1770\n",
            "Epoch 94/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9988 - loss: 0.0050 - val_accuracy: 0.9556 - val_loss: 0.1934\n",
            "Epoch 95/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9984 - loss: 0.0085 - val_accuracy: 0.9488 - val_loss: 0.2285\n",
            "Epoch 96/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9959 - loss: 0.0139 - val_accuracy: 0.9600 - val_loss: 0.1739\n",
            "Epoch 97/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9959 - loss: 0.0166 - val_accuracy: 0.9606 - val_loss: 0.1787\n",
            "Epoch 98/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0056 - val_accuracy: 0.9513 - val_loss: 0.2391\n",
            "Epoch 99/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9926 - loss: 0.0225 - val_accuracy: 0.9575 - val_loss: 0.1927\n",
            "Epoch 100/100\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9982 - loss: 0.0073 - val_accuracy: 0.9600 - val_loss: 0.1896\n"
          ]
        }
      ],
      "source": [
        "# 7. Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOXR5POVXAlE",
        "outputId": "8fad0896-bd18-4f2c-a25e-181f42acf3ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9657\n"
          ]
        }
      ],
      "source": [
        "# 8. Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlQ11mf3XQT1",
        "outputId": "2ac458a3-2326-4f16-8c2b-a1c5e4051421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "model.save(\"DNN.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw25kXZtXC00",
        "outputId": "7488ab95-028e-420b-94c7-96587042682e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9639 - loss: 0.1981\n",
            "Test Accuracy: 0.9657\n"
          ]
        }
      ],
      "source": [
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFX4ZQayXoRa",
        "outputId": "6bd4a55d-edc1-4d9c-f8be-cdd9c31f74c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
          ]
        }
      ],
      "source": [
        "# 9. Make predictions (optional)\n",
        "y_pred = model.predict(X_test)\n",
        "predicted_labels = label_encoder.inverse_transform(np.argmax(y_pred, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBcPctkU_UdM",
        "outputId": "da15a688-7549-4c5b-ef2d-44aa547ec1dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\n",
            "--- Random Sample Test ---\n",
            "True Letter: E\n",
            "Predicted Letter: E\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def random_sample_predict(model, scaler, label_encoder, X_test, y_test):\n",
        "    # Pick a random index\n",
        "    idx = random.randint(0, len(X_test) - 1)\n",
        "\n",
        "    # Select random sample\n",
        "    sample = X_test[idx].reshape(1, -1)\n",
        "    true_label = np.argmax(y_test[idx])\n",
        "    true_letter = label_encoder.inverse_transform([true_label])[0]\n",
        "\n",
        "    # Predict\n",
        "    prediction = model.predict(sample)\n",
        "    predicted_class = np.argmax(prediction, axis=1)\n",
        "    predicted_letter = label_encoder.inverse_transform(predicted_class)[0]\n",
        "\n",
        "    print(f\"\\n--- Random Sample Test ---\")\n",
        "    print(f\"True Letter: {true_letter}\")\n",
        "    print(f\"Predicted Letter: {predicted_letter}\")\n",
        "\n",
        "# Call this function after model training\n",
        "random_sample_predict(model, scaler, label_encoder, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggxJX5bCZ3f-"
      },
      "source": [
        "### DL LAB 2B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXitG_WRAujV",
        "outputId": "b88cb07f-298b-4a66-c1bc-8fca3664ecea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 - 4s - 891ms/step - accuracy: 0.3000 - loss: 0.6962\n",
            "Epoch 2/20\n",
            "5/5 - 1s - 115ms/step - accuracy: 0.6000 - loss: 0.6911\n",
            "Epoch 3/20\n",
            "5/5 - 0s - 11ms/step - accuracy: 0.8000 - loss: 0.6861\n",
            "Epoch 4/20\n",
            "5/5 - 0s - 12ms/step - accuracy: 0.9000 - loss: 0.6808\n",
            "Epoch 5/20\n",
            "5/5 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.6741\n",
            "Epoch 6/20\n",
            "5/5 - 0s - 12ms/step - accuracy: 1.0000 - loss: 0.6637\n",
            "Epoch 7/20\n",
            "5/5 - 0s - 12ms/step - accuracy: 1.0000 - loss: 0.6487\n",
            "Epoch 8/20\n",
            "5/5 - 0s - 28ms/step - accuracy: 1.0000 - loss: 0.6297\n",
            "Epoch 9/20\n",
            "5/5 - 0s - 12ms/step - accuracy: 1.0000 - loss: 0.6073\n",
            "Epoch 10/20\n",
            "5/5 - 0s - 12ms/step - accuracy: 1.0000 - loss: 0.5529\n",
            "Epoch 11/20\n",
            "5/5 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.4892\n",
            "Epoch 12/20\n",
            "5/5 - 0s - 12ms/step - accuracy: 1.0000 - loss: 0.4003\n",
            "Epoch 13/20\n",
            "5/5 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.2851\n",
            "Epoch 14/20\n",
            "5/5 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.1720\n",
            "Epoch 15/20\n",
            "5/5 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.1115\n",
            "Epoch 16/20\n",
            "5/5 - 0s - 12ms/step - accuracy: 1.0000 - loss: 0.1279\n",
            "Epoch 17/20\n",
            "5/5 - 0s - 11ms/step - accuracy: 0.9000 - loss: 0.1126\n",
            "Epoch 18/20\n",
            "5/5 - 0s - 11ms/step - accuracy: 1.0000 - loss: 0.0509\n",
            "Epoch 19/20\n",
            "5/5 - 0s - 12ms/step - accuracy: 1.0000 - loss: 0.0322\n",
            "Epoch 20/20\n",
            "5/5 - 0s - 17ms/step - accuracy: 1.0000 - loss: 0.0320\n",
            "\n",
            "Review Sentiment: Positive (Score: 0.9701)\n",
            "\n",
            "Review Sentiment: Negative (Score: 0.0706)\n",
            "\n",
            "Review Sentiment: Negative (Score: 0.1041)\n",
            "\n",
            "Review Sentiment: Positive (Score: 0.9948)\n"
          ]
        }
      ],
      "source": [
        "# 1. Import Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# 2. Create a small custom dataset (manually for simplicity)\n",
        "texts = [\n",
        "    \"The movie was fantastic and thrilling\",\n",
        "    \"I hated the movie, it was boring and bad\",\n",
        "    \"An excellent movie with brilliant performances\",\n",
        "    \"The film was dull and too long\",\n",
        "    \"Loved the story and the acting was amazing\",\n",
        "    \"Terrible movie, complete waste of time\",\n",
        "    \"What a masterpiece, loved every moment\",\n",
        "    \"Worst movie ever, so disappointed\",\n",
        "    \"Absolutely stunning, a wonderful experience\",\n",
        "    \"I regret watching this movie, very bad\"\n",
        "]\n",
        "\n",
        "labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1 = Positive, 0 = Negative\n",
        "\n",
        "# 3. Tokenize the texts\n",
        "max_words = 1000\n",
        "max_len = 20\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# 4. Build the Model\n",
        "model = keras.Sequential([\n",
        "    layers.Embedding(input_dim=max_words, output_dim=64, input_length=max_len),\n",
        "    layers.Bidirectional(layers.LSTM(32)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# 5. Compile Model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 6. Train the Model\n",
        "model.fit(padded_sequences, np.array(labels), epochs=20, batch_size=2, verbose=2)\n",
        "\n",
        "# 7. Real-time Prediction Function\n",
        "def predict_sentiment(review):\n",
        "    seq = tokenizer.texts_to_sequences([review])\n",
        "    padded = pad_sequences(seq, maxlen=max_len, padding='post')\n",
        "    pred = model.predict(padded, verbose=0)[0][0]\n",
        "    sentiment = \"Positive\" if pred >= 0.5 else \"Negative\"\n",
        "    print(f\"\\nReview Sentiment: {sentiment} (Score: {pred:.4f})\")\n",
        "\n",
        "# 8. Real-time Testing\n",
        "sample_review1 = \"The movie was fantastic! I really loved the performances.\"\n",
        "predict_sentiment(sample_review1)\n",
        "\n",
        "sample_review2 = \"The film was boring and too long. Not good at all.\"\n",
        "predict_sentiment(sample_review2)\n",
        "\n",
        "sample_review3 = \"I absolutely hated this movie. Worst experience ever.\"\n",
        "predict_sentiment(sample_review3)\n",
        "\n",
        "sample_review4 = \"An excellent masterpiece. Great story and acting.\"\n",
        "predict_sentiment(sample_review4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3Rf5SzkDNtj",
        "outputId": "59dac180-72f7-4191-c1ad-ce938197bab7"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.6802 - loss: 0.5678 - val_accuracy: 0.8520 - val_loss: 0.3495\n",
            "Epoch 2/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.9083 - loss: 0.2372 - val_accuracy: 0.8472 - val_loss: 0.3714\n",
            "Epoch 3/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.9327 - loss: 0.1766 - val_accuracy: 0.8668 - val_loss: 0.3616\n",
            "Epoch 4/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.9615 - loss: 0.1122 - val_accuracy: 0.8690 - val_loss: 0.3863\n",
            "Epoch 5/5\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - accuracy: 0.9737 - loss: 0.0762 - val_accuracy: 0.8522 - val_loss: 0.4288\n",
            "\n",
            "Review Sentiment: Positive (Score: 0.9436)\n",
            "\n",
            "Review Sentiment: Positive (Score: 0.5666)\n",
            "\n",
            "Review Sentiment: Negative (Score: 0.1899)\n",
            "\n",
            "Review Sentiment: Positive (Score: 0.7655)\n"
          ]
        }
      ],
      "source": [
        "# 1. Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# 2. Load the IMDB dataset (with raw text)\n",
        "imdb = keras.datasets.imdb\n",
        "\n",
        "# Set vocabulary size\n",
        "vocab_size = 10000\n",
        "\n",
        "# Load dataset (already preprocessed as integers)\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "# 3. Decode function to get back text\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = {value: key for key, value in word_index.items()}\n",
        "\n",
        "def decode_review(text_ints):\n",
        "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in text_ints])\n",
        "\n",
        "# 4. Prepare data (pad sequences)\n",
        "maxlen = 200\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# 5. Build model\n",
        "model = keras.Sequential([\n",
        "    layers.Embedding(vocab_size, 64, input_length=maxlen),\n",
        "    layers.Bidirectional(layers.LSTM(64)),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# 6. Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 7. Train model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# 8. Real-time testing function\n",
        "def predict_sentiment_text(model, review_text):\n",
        "    # 8.1 Preprocessing: convert review to integers\n",
        "    words = review_text.lower().split()\n",
        "    review_seq = []\n",
        "    for word in words:\n",
        "        idx = word_index.get(word, 2)  # 2 is for unknown words\n",
        "        review_seq.append(idx)\n",
        "\n",
        "    review_seq = pad_sequences([review_seq], maxlen=maxlen)\n",
        "\n",
        "    pred = model.predict(review_seq, verbose=0)[0][0]\n",
        "    sentiment = \"Positive\" if pred >= 0.5 else \"Negative\"\n",
        "    print(f\"\\nReview Sentiment: {sentiment} (Score: {pred:.4f})\")\n",
        "\n",
        "# 9. Real examples\n",
        "sample_review1 = \"The movie was fantastic! I really loved the performances.\"\n",
        "predict_sentiment_text(model, sample_review1)\n",
        "\n",
        "sample_review2 = \"The film was boring and too long. Not good at all.\"\n",
        "predict_sentiment_text(model, sample_review2)\n",
        "\n",
        "sample_review3 = \"it is so disappointing.\"\n",
        "predict_sentiment_text(model, sample_review3)\n",
        "\n",
        "sample_review4 = \"An excellent movie. Great direction and amazing acting!\"\n",
        "predict_sentiment_text(model, sample_review4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekyTq-yCEqms"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, let's break down your entire notebook, cell by cell, in simple terms. I'll explain the code and the important Deep Learning (DL) words you'll encounter. This will be great for your practical!\n",
        "\n",
        "**Core Idea of Deep Learning:**\n",
        "Imagine you're trying to teach a computer to recognize things (like letters or movie sentiment). Instead of writing exact rules, you show it lots of examples and let it learn the patterns itself. A \"Deep Neural Network\" (DNN) is like a computer's brain with many layers that help it learn these complex patterns.\n",
        "\n",
        "---\n",
        "\n",
        "**File: Ass_2.ipynb**\n",
        "\n",
        "---\n",
        "\n",
        "**PART 1: DL LAB 2A - Recognizing Handwritten Letters**\n",
        "*(Goal: Teach the computer to identify which letter of the alphabet (A-Z) an image represents, based on some numerical features of that letter's image.)*\n",
        "\n",
        "**Cell 0: Markdown**\n",
        "```markdown\n",
        "### DL LAB 2A\n",
        "```\n",
        "*   **What it is:** Just a title for this section.\n",
        "*   **Terminology:**\n",
        "    *   **Markdown:** A simple way to format text (like headings, bold, lists).\n",
        "\n",
        "**Cell 1: Imports**\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "```\n",
        "*   **What it does:** This cell brings in all the tools (libraries) needed for this part.\n",
        "*   **Terminology:**\n",
        "    *   `pandas` (`pd`): A library for working with data in tables (like Excel spreadsheets, called DataFrames).\n",
        "    *   `numpy` (`np`): For heavy-duty math and working with lists of numbers (arrays).\n",
        "    *   `sklearn` (Scikit-learn): A popular library for general Machine Learning tasks.\n",
        "        *   `train_test_split`: A function to split your data into a \"training set\" (to teach the model) and a \"test set\" (to see how well it learned on new data).\n",
        "        *   `LabelEncoder`: Converts text labels (like 'A', 'B', 'C') into numbers (0, 1, 2). Computers prefer numbers.\n",
        "        *   `StandardScaler`: A tool to rescale your numerical features so they are all on a similar range (e.g., around 0 with a standard spread). This helps the DNN learn better.\n",
        "    *   `tensorflow.keras`: TensorFlow is a big library for Deep Learning. Keras is a user-friendly way to build models with TensorFlow.\n",
        "        *   `Sequential`: A type of model where you stack layers one after another, like building blocks.\n",
        "        *   `Dense`: A basic type of layer in a neural network where every \"neuron\" (processing unit) in this layer is connected to every neuron in the previous layer.\n",
        "        *   `to_categorical`: A function to convert number labels (like 0, 1, 2) into a special format called \"one-hot encoding\" (e.g., 0 becomes `[1,0,0]`, 1 becomes `[0,1,0]`). This is often needed for classifying into multiple categories.\n",
        "\n",
        "**Cell 2: Load Dataset**\n",
        "```python\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\"\n",
        "columns = [...] # list of column names\n",
        "data = pd.read_csv(url, names=columns)\n",
        "```\n",
        "*   **What it does:** Downloads the letter recognition dataset from the internet and loads it into a pandas DataFrame (table). It also assigns names to the columns.\n",
        "*   **Terminology:**\n",
        "    *   **Dataset:** A collection of data used for training and testing. Here, it's data about letter images.\n",
        "    *   **URL:** A web address.\n",
        "    *   `pd.read_csv()`: Pandas function to read data from a CSV (Comma Separated Values) file.\n",
        "\n",
        "**Cell 3: Separate Features and Labels**\n",
        "```python\n",
        "X = data.drop('letter', axis=1).values\n",
        "y = data['letter'].values\n",
        "```\n",
        "*   **What it does:** Splits the data into two parts:\n",
        "    *   `X`: The **features** (the 16 numerical measurements for each letter image). These are the inputs the model will use to make predictions.\n",
        "    *   `y`: The **labels** (the actual letter 'A' through 'Z'). This is what we want the model to predict.\n",
        "*   **Terminology:**\n",
        "    *   **Features (X):** The input variables or characteristics used for prediction.\n",
        "    *   **Labels (y) / Target Variable:** The output variable you are trying to predict.\n",
        "\n",
        "**Cell 4: Encode Labels**\n",
        "```python\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "y_categorical = to_categorical(y_encoded)\n",
        "```\n",
        "*   **What it does:** Converts the letter labels into a format suitable for the DNN.\n",
        "    1.  `LabelEncoder()`: Changes letters ('A', 'B', ...) to numbers (0, 1, ...).\n",
        "    2.  `to_categorical()`: Changes these numbers into **one-hot encoded** vectors. For 26 letters:\n",
        "        *   'A' (or 0) becomes `[1, 0, 0, ..., 0]` (a list of 26 numbers with 1 at the 0th position).\n",
        "        *   'B' (or 1) becomes `[0, 1, 0, ..., 0]` (1 at the 1st position).\n",
        "*   **Terminology:**\n",
        "    *   **One-Hot Encoding:** A way to represent categorical data (like letters) as binary vectors. Each category gets its own position in the vector, which is 1 if the sample belongs to that category, and 0 otherwise. Important for `categorical_crossentropy` loss.\n",
        "\n",
        "**Cell 5: Train-Test Split**\n",
        "```python\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42, stratify=y_categorical)\n",
        "```\n",
        "*   **What it does:** Divides the data:\n",
        "    *   80% for **training** (`X_train`, `y_train`): The model learns from this.\n",
        "    *   20% for **testing** (`X_test`, `y_test`): Used to evaluate how well the model performs on unseen data.\n",
        "*   **Terminology:**\n",
        "    *   `test_size=0.2`: 20% of data for testing.\n",
        "    *   `random_state=42`: Ensures the split is the same every time you run the code (for reproducibility).\n",
        "    *   `stratify=y_categorical`: Tries to keep the same proportion of each letter in both training and testing sets.\n",
        "\n",
        "**Cell 6: Feature Scaling**\n",
        "```python\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "```\n",
        "*   **What it does:** Rescales the numerical features in `X_train` and `X_test`.\n",
        "    *   `fit_transform` on `X_train`: Learns the scaling parameters (mean, standard deviation) from the training data and then applies the scaling.\n",
        "    *   `transform` on `X_test`: Applies the scaling learned from the training data. (Important: Don't `fit` on test data to avoid data leakage).\n",
        "*   **Terminology:**\n",
        "    *   **Feature Scaling:** Making sure all input features have a similar range of values. This helps the DNN learn more efficiently. `StandardScaler` makes features have roughly a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "**Cell 7: Build the DNN Model**\n",
        "```python\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(16,)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(26, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "```\n",
        "*   **What it does:** Defines the architecture (structure) of our Deep Neural Network.\n",
        "    *   `Sequential([...])`: We're building a model layer by layer.\n",
        "    *   `Dense(128, activation='relu', input_shape=(16,))`: The first **hidden layer**.\n",
        "        *   `Dense`: A fully connected layer.\n",
        "        *   `128`: It has 128 \"neurons\" or units.\n",
        "        *   `activation='relu'`: **ReLU (Rectified Linear Unit)** is an **activation function**. It decides if a neuron should be \"activated\" or not. ReLU is simple: if input is positive, output is the input; if negative, output is 0.\n",
        "        *   `input_shape=(16,)`: This layer expects 16 input features (our letter image features). Only needed for the first layer.\n",
        "    *   `Dense(64, activation='relu')`: The second hidden layer with 64 neurons and ReLU activation.\n",
        "    *   `Dense(26, activation='softmax')`: The **output layer**.\n",
        "        *   `26`: It has 26 neurons, one for each letter of the alphabet.\n",
        "        *   `activation='softmax'`: **Softmax** activation is used for multi-class classification. It converts the raw outputs of the neurons into probabilities, ensuring they all add up to 1. The neuron with the highest probability is the model's predicted letter.\n",
        "    *   `model.compile(...)`: Configures how the model will learn.\n",
        "        *   `optimizer='adam'`: **Adam** is an efficient **optimizer**. The optimizer's job is to adjust the model's internal \"knobs\" (weights) to reduce mistakes.\n",
        "        *   `loss='categorical_crossentropy'`: The **loss function** measures how \"wrong\" the model's predictions are compared to the true labels (which are one-hot encoded). The model tries to minimize this loss.\n",
        "        *   `metrics=['accuracy']`: We want to track **accuracy** (the percentage of correctly classified letters) during training.\n",
        "*   **Terminology:**\n",
        "    *   **Neural Network / DNN:** A model inspired by the human brain, with layers of interconnected nodes (neurons). \"Deep\" means it has multiple hidden layers.\n",
        "    *   **Layer:** A stage of computation in the network.\n",
        "    *   **Hidden Layer:** Layers between the input and output layers. They learn complex features.\n",
        "    *   **Output Layer:** The final layer that gives the prediction.\n",
        "    *   **Neuron/Unit:** A small computational unit within a layer.\n",
        "    *   **Activation Function (ReLU, Softmax):** A function applied to the output of each neuron to introduce non-linearity, allowing the network to learn complex patterns.\n",
        "    *   **Optimizer (Adam):** An algorithm that adjusts the model's parameters (weights) to minimize the loss function.\n",
        "    *   **Loss Function (Categorical Crossentropy):** A way to measure the error or \"badness\" of the model's predictions for multi-class problems.\n",
        "    *   **Metrics (Accuracy):** How we evaluate the model's performance.\n",
        "\n",
        "**Cell 8: Train the Model**\n",
        "```python\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1)\n",
        "```\n",
        "*   **What it does:** This is where the actual learning happens!\n",
        "    *   `model.fit()`: Trains the model using the training data (`X_train`, `y_train`).\n",
        "    *   `epochs=100`: An **epoch** is one complete pass through the entire training dataset. Here, it goes through the data 100 times.\n",
        "    *   `batch_size=32`: The model processes the training data in small groups (batches) of 32 samples at a time before updating its weights.\n",
        "    *   `validation_split=0.1`: It sets aside 10% of the training data to use as **validation data**. After each epoch, the model's performance is checked on this validation data. This helps monitor if the model is **overfitting** (learning the training data too well but not generalizing to new data).\n",
        "*   **Terminology:**\n",
        "    *   **Training:** The process of teaching the model by showing it examples and letting it adjust its weights.\n",
        "    *   **Epoch:** One full iteration over the entire training dataset.\n",
        "    *   **Batch Size:** The number of training samples processed before the model's weights are updated.\n",
        "    *   **Validation Data:** A subset of training data used to tune hyperparameters and monitor for overfitting during training.\n",
        "    *   **Overfitting:** When a model performs very well on training data but poorly on unseen (test or validation) data. It has \"memorized\" the training set instead of learning general patterns.\n",
        "\n",
        "**Cell 9: Evaluate the Model**\n",
        "```python\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "```\n",
        "*   **What it does:** Tests the trained model on the `X_test` and `y_test` (data it has never seen before) to see how well it generalized.\n",
        "*   **Terminology:**\n",
        "    *   **Evaluation:** Assessing the model's performance on unseen data.\n",
        "    *   `Test Accuracy`: The accuracy on the test set, a good indicator of how the model will perform in the real world.\n",
        "\n",
        "**Cell 10: Save the Model**\n",
        "```python\n",
        "model.save(\"DNN.h5\")\n",
        "```\n",
        "*   **What it does:** Saves the entire trained model (architecture, weights, optimizer state) to a file named `DNN.h5`. This allows you to load and use it later without retraining.\n",
        "*   **Terminology:**\n",
        "    *   **Model Saving:** Storing the trained model for later use. `.h5` is a common format.\n",
        "\n",
        "**Cell 11: Evaluate Model (Again)**\n",
        "```python\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n",
        "```\n",
        "*   **What it does:** Same as Cell 9, just showing evaluation again. The `verbose=0` in Cell 9 just meant \"don't print progress bars during evaluation.\" This cell will print them.\n",
        "\n",
        "**Cell 12: Make Predictions**\n",
        "```python\n",
        "y_pred = model.predict(X_test)\n",
        "predicted_labels = label_encoder.inverse_transform(np.argmax(y_pred, axis=1))\n",
        "```\n",
        "*   **What it does:** Uses the trained model to predict letters for the test data.\n",
        "    *   `model.predict(X_test)`: Gets the raw probability outputs from the softmax layer for each sample in `X_test`.\n",
        "    *   `np.argmax(y_pred, axis=1)`: For each sample, finds the index (0-25) of the neuron with the highest probability. This is the predicted numerical label.\n",
        "    *   `label_encoder.inverse_transform(...)`: Converts these predicted numerical labels back to actual letters ('A', 'B', ...).\n",
        "*   **Terminology:**\n",
        "    *   **Prediction/Inference:** Using the trained model to make predictions on new data.\n",
        "\n",
        "**Cell 13: Random Sample Test**\n",
        "```python\n",
        "import random\n",
        "# ... (function definition) ...\n",
        "random_sample_predict(model, scaler, label_encoder, X_test, y_test)\n",
        "```\n",
        "*   **What it does:** A custom function that picks a random sample from the test set, shows its true letter, and what the model predicted for it. A nice way to see the model in action.\n",
        "\n",
        "---\n",
        "\n",
        "**PART 2: DL LAB 2B - Movie Review Sentiment Analysis**\n",
        "*(Goal: Teach the computer to read a movie review and decide if it's \"positive\" or \"negative\".)*\n",
        "\n",
        "**Cell 14: Markdown**\n",
        "```markdown\n",
        "### DL LAB 2B\n",
        "```\n",
        "*   Just a title for this new section.\n",
        "\n",
        "**Cell 15: Sentiment Analysis with a Small Custom Dataset**\n",
        "*   **What it does:** This cell builds a model to classify short text snippets as positive or negative using a very small, manually created dataset.\n",
        "*   **Key Steps & Terminology:**\n",
        "    *   `texts`: A list of example sentences (movie reviews).\n",
        "    *   `labels`: 0 for negative, 1 for positive. This is **binary classification** (two classes).\n",
        "    *   `Tokenizer(num_words=max_words, oov_token=\"<OOV>\")`:\n",
        "        *   **Tokenizer:** Tool to break text into individual words (tokens) and convert them into numbers (sequences).\n",
        "        *   `num_words`: Only considers the `max_words` most frequent words (vocabulary size).\n",
        "        *   `oov_token=\"<OOV>\"`: A special token for \"Out Of Vocabulary\" words (words the tokenizer hasn't seen during training).\n",
        "    *   `tokenizer.fit_on_texts(texts)`: Learns the vocabulary from your `texts`.\n",
        "    *   `sequences = tokenizer.texts_to_sequences(texts)`: Converts each review into a sequence of numbers (each number representing a word).\n",
        "    *   `padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')`:\n",
        "        *   **Padding:** Makes all word sequences the same length (`max_len`) by adding zeros (usually at the end - `padding='post'`). DNNs need fixed-size inputs.\n",
        "    *   **Model Architecture:**\n",
        "        *   `layers.Embedding(input_dim=max_words, output_dim=64, input_length=max_len)`:\n",
        "            *   **Embedding Layer:** Crucial for text! It learns a dense vector (a list of numbers, here 64 numbers long) for each word in your vocabulary. These vectors capture the \"meaning\" or context of words (e.g., \"good\" and \"great\" might have similar vectors).\n",
        "        *   `layers.Bidirectional(layers.LSTM(32))`:\n",
        "            *   **LSTM (Long Short-Term Memory):** A special type of **Recurrent Neural Network (RNN)** layer. RNNs are good for sequence data (like text or time series) because they have a \"memory\" – they can consider previous words in a sentence when processing the current word.\n",
        "            *   **Bidirectional:** The LSTM processes the sequence from left-to-right AND right-to-left, giving it more context from both directions.\n",
        "        *   `layers.Dense(32, activation='relu')`: A standard hidden layer.\n",
        "        *   `layers.Dense(1, activation='sigmoid')`: The output layer for binary classification.\n",
        "            *   `1`: One neuron because there are only two outcomes (positive/negative).\n",
        "            *   `activation='sigmoid'`: **Sigmoid** activation squashes the output to a probability between 0 and 1. If > 0.5, often classified as positive; if < 0.5, as negative.\n",
        "    *   `model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])`:\n",
        "        *   `loss='binary_crossentropy'`: The appropriate loss function for binary (0/1) classification problems.\n",
        "    *   `model.fit(...)`: Trains the model on this small dataset.\n",
        "    *   `predict_sentiment(review)`: A function to take a new review, preprocess it (tokenize, pad), and predict its sentiment.\n",
        "\n",
        "**Cell 16: Sentiment Analysis with the IMDB Dataset**\n",
        "*   **What it does:** This cell does the same task (sentiment analysis) but uses a much larger, standard dataset called IMDB, which Keras can load directly.\n",
        "*   **Key Differences & Terminology:**\n",
        "    *   `imdb = keras.datasets.imdb`: Loads the IMDB dataset module.\n",
        "    *   `(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)`:\n",
        "        *   Loads the IMDB data. The reviews (`x_train`, `x_test`) are already preprocessed into sequences of word IDs (integers). `y_train`, `y_test` are 0 or 1.\n",
        "    *   `word_index = imdb.get_word_index()`: Gets a dictionary mapping words to their integer IDs.\n",
        "    *   `reverse_word_index`: Creates a dictionary to map IDs back to words.\n",
        "    *   `decode_review(text_ints)`: A function to convert a sequence of word IDs back into readable text.\n",
        "    *   `x_train = pad_sequences(x_train, maxlen=maxlen)`: Pads the IMDB review sequences to a fixed length (`maxlen`).\n",
        "    *   **Model:** Similar architecture (Embedding, Bidirectional LSTM, Dense layers) but configured for the IMDB dataset's `vocab_size` and chosen `maxlen`.\n",
        "    *   `model.fit(x_train, y_train, ...)`: Trains on the larger IMDB dataset.\n",
        "    *   `predict_sentiment_text(model, review_text)`:\n",
        "        *   This function is more complex because it takes raw text.\n",
        "        *   It manually converts words to their IDs using `word_index`.\n",
        "        *   Then pads the sequence and predicts.\n",
        "\n",
        "**Cell 17: Empty Code Cell**\n",
        "*   This cell is empty, nothing happens here.\n",
        "\n",
        "---\n",
        "\n",
        "**Key Takeaways for Your Practical:**\n",
        "\n",
        "1.  **Understand the Goal:** Are you doing multi-class classification (like letters A-Z) or binary classification (like positive/negative)?\n",
        "2.  **Data Preprocessing is Key:**\n",
        "    *   **Numerical Data (Lab 2A):** Scaling (e.g., `StandardScaler`).\n",
        "    *   **Categorical Labels (Lab 2A):** `LabelEncoder` then `to_categorical` (one-hot encoding).\n",
        "    *   **Text Data (Lab 2B):** `Tokenizer`, then convert to sequences, then `pad_sequences`.\n",
        "3.  **Model Building Blocks (Keras `Sequential` model):**\n",
        "    *   **Input Layer:** Defined by `input_shape` in the first layer.\n",
        "    *   **Hidden Layers:** `Dense` layers (for general patterns), `LSTM` (for sequences/text).\n",
        "    *   **Output Layer:**\n",
        "        *   `Dense(num_classes, activation='softmax')` for multi-class.\n",
        "        *   `Dense(1, activation='sigmoid')` for binary.\n",
        "    *   **Activation Functions:** `relu` (common for hidden layers), `softmax` (multi-class output), `sigmoid` (binary output).\n",
        "4.  **Compilation (`model.compile`):**\n",
        "    *   **Optimizer:** `adam` is a good default.\n",
        "    *   **Loss Function:**\n",
        "        *   `categorical_crossentropy` for multi-class (with one-hot encoded labels).\n",
        "        *   `binary_crossentropy` for binary.\n",
        "    *   **Metrics:** `accuracy` is common.\n",
        "5.  **Training (`model.fit`):**\n",
        "    *   `epochs`: How many times to go through the data.\n",
        "    *   `batch_size`: How many samples to process before an update.\n",
        "    *   `validation_split` or `validation_data`: To monitor for overfitting.\n",
        "6.  **Evaluation (`model.evaluate`):** Use the test set to get a final, unbiased performance measure.\n",
        "7.  **Prediction (`model.predict`):** Use the trained model on new, unseen data.\n",
        "\n",
        "**Simple Analogies:**\n",
        "\n",
        "*   **DNN:** A team of specialists. Each layer specializes in finding certain types of patterns. Early layers find simple patterns, later layers combine them into complex ones.\n",
        "*   **Epoch:** Reading a textbook once. Multiple epochs mean re-reading it to understand better.\n",
        "*   **Batch Size:** Studying in chunks. Instead of reading the whole textbook at once, you study a few pages (batch) then try to recall/update your understanding.\n",
        "*   **Loss Function:** Your teacher telling you how many questions you got wrong on a quiz. You try to study so you get fewer wrong next time.\n",
        "*   **Optimizer:** The study method you use to improve your quiz scores.\n",
        "*   **Embedding Layer (for text):** A super-smart dictionary that knows not just what words mean, but how they relate to each other.\n",
        "\n",
        "Good luck with your practical! Focus on understanding the *purpose* of each step and the main Keras components."
      ],
      "metadata": {
        "id": "EmYqRZ3Yi6VS"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "5fRGY3TUZ-GP"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}